{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd0ef7f9a8012d9131766e31894c279374cc63c73121ed4db3b9e67a294a4bf0e74",
   "display_name": "Python 3.8.3 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path, dataset):\n",
    "    with open(data_path + '20news_' + dataset + '_tfidf.txt') as f:\n",
    "        labels = []\n",
    "        tf_idf = []\n",
    "        doc_idx = []\n",
    "        vocab_idx = []\n",
    "        lines = f.read().splitlines()\n",
    "        dataset_size = len(lines)\n",
    "        for line_idx, line in enumerate(lines):\n",
    "            features = line.split('<fff>')\n",
    "            labels.append(int(features[0]))\n",
    "            for word in features[2].split():\n",
    "                tf_idf.append(float(word.split(':')[1]))\n",
    "                doc_idx.append(line_idx)\n",
    "                vocab_idx.append(int(word.split(':')[0]))\n",
    "    with open(data_path + 'words_idfs.txt') as f:\n",
    "        vocab_size = len(f.read().splitlines())\n",
    "    return np.array(labels), np.array(tf_idf), np.array(doc_idx), np.array(vocab_idx), dataset_size, vocab_size"
   ]
  },
  {
   "source": [
    "# K Means"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_with_KMeans(path):\n",
    "    _, tf_idf, doc_idx, vocab_idx, dataset_size, vocab_size = load_data(path, dataset = 'full')\n",
    "\n",
    "    X = csr_matrix((tf_idf, (doc_idx, vocab_idx)), shape = (dataset_size, vocab_size))\n",
    "    print('--------------')\n",
    "    kmeans = KMeans(\n",
    "        n_clusters = 20,\n",
    "        init = 'random',\n",
    "        n_init = 5, #differently init centroids\n",
    "        tol = 1e-3, #threshold\n",
    "        random_state = 2018 #seed\n",
    "    ).fit(X)\n",
    "    return kmeans\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--------------\n"
     ]
    }
   ],
   "source": [
    "path = '..\\\\datasets\\\\20news-bydate\\\\'\n",
    "k_means = clustering_with_KMeans(path = path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 0.00000000e+00, 4.20829291e-04, ...,\n",
       "        6.77217674e-04, 5.21527356e-05, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        7.07271997e-04, 5.50333086e-05, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        8.10727782e-04, 7.08597261e-05, 0.00000000e+00],\n",
       "       ...,\n",
       "       [2.07833837e-04, 7.97894043e-04, 6.95846695e-05, ...,\n",
       "        5.36674538e-04, 4.69161363e-05, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.03462005e-03, ...,\n",
       "        7.90950011e-04, 5.96664713e-05, 0.00000000e+00],\n",
       "       [0.00000000e+00, 1.07023418e-03, 0.00000000e+00, ...,\n",
       "        7.62676436e-04, 5.96495460e-05, 0.00000000e+00]])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "k_means.cluster_centers_"
   ]
  },
  {
   "source": [
    "# Linear SVM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_predicted, y_expected):\n",
    "    matches = np.equal(y_predicted, y_expected)\n",
    "    accuracy = np.sum(matches.astype('float')) / y_expected.size\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifying_with_linear_SVMs(path):\n",
    "    y_train, tf_idf, doc_idx, vocab_idx, dataset_size, vocab_size = load_data(path, dataset = 'train')\n",
    "    X_train = csr_matrix((tf_idf, (doc_idx, vocab_idx)), shape = (dataset_size, vocab_size))\n",
    "\n",
    "    classifier = LinearSVC(\n",
    "        C = 10.0,\n",
    "        tol = 0.001,\n",
    "    ).fit(X_train, y_train)\n",
    "    \n",
    "    y_test, tf_idf, doc_idx, vocab_idx, dataset_size, vocab_size = load_data(path, dataset = 'test')\n",
    "    X_test = csr_matrix((tf_idf, (doc_idx, vocab_idx)), shape = (dataset_size, vocab_size))\n",
    "    y_predicted = classifier.predict(X_test)\n",
    "    accuracy = compute_accuracy(y_predicted = y_predicted, y_expected = y_test)\n",
    "    print('accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "accuracy:  0.8224907063197026\n"
     ]
    }
   ],
   "source": [
    "classifying_with_linear_SVMs(path)"
   ]
  },
  {
   "source": [
    "# Kernel SVM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifying_with_kernel_SVMs(path):\n",
    "    y_train, tf_idf, doc_idx, vocab_idx, dataset_size, vocab_size = load_data(path, dataset = 'train')\n",
    "    X_train = csr_matrix((tf_idf, (doc_idx, vocab_idx)), shape = (dataset_size, vocab_size))\n",
    "\n",
    "    classifier = SVC(\n",
    "        C = 50.0,\n",
    "        kernel = 'rbf',\n",
    "        gamma = 0.1,\n",
    "        tol = 0.001,\n",
    "    ).fit(X_train, y_train)\n",
    "    \n",
    "    y_test, tf_idf, doc_idx, vocab_idx, dataset_size, vocab_size = load_data(path, dataset = 'test')\n",
    "    X_test = csr_matrix((tf_idf, (doc_idx, vocab_idx)), shape = (dataset_size, vocab_size))\n",
    "    y_predicted = classifier.predict(X_test)\n",
    "    accuracy = compute_accuracy(y_predicted = y_predicted, y_expected = y_test)\n",
    "    print('accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "accuracy:  0.8194370685077005\n"
     ]
    }
   ],
   "source": [
    "classifying_with_kernel_SVMs(path)"
   ]
  }
 ]
}