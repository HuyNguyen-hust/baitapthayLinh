{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd0ef7f9a8012d9131766e31894c279374cc63c73121ed4db3b9e67a294a4bf0e74",
   "display_name": "Python 3.8.3 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data_and_vocab():\n",
    "  def collect_data_from(parent_path, newsgroup_list, word_count = None):\n",
    "    data = []\n",
    "    for group_id, newsgroup in enumerate(newsgroup_list):\n",
    "      dir_path = parent_path + '\\\\' + newsgroup + '\\\\'\n",
    "\n",
    "      files = [(filename, dir_path + filename) for filename in os.listdir(dir_path) if os.path.isfile(dir_path + filename)]\n",
    "\n",
    "      files.sort()\n",
    "      label = group_id\n",
    "      print('Processing: {}-{}'.format(group_id, newsgroup))\n",
    "\n",
    "      for file_name, filepath in files:\n",
    "        with open(filepath) as f:\n",
    "          text = f.read().lower()\n",
    "          words = re.split('\\W+', text)\n",
    "          if word_count is not None:\n",
    "            for word in words:\n",
    "              word_count[word] += 1\n",
    "          content = ' '.join(words)\n",
    "          assert len(content.splitlines()) == 1\n",
    "          data.append(str(label) + '<fff>' + file_name + '<fff>' + content)\n",
    "    return data\n",
    "\n",
    "  word_count = defaultdict(int)\n",
    "  path = '..\\\\datasets\\\\20news-bydate\\\\'\n",
    "  parts = [path + dirname + '\\\\' for dirname in os.listdir(path) if not os.path.isfile(path + dirname)]\n",
    "\n",
    "  train_path, test_path = (parts[0], parts[1]) if 'train' in parts[0] else (parts[1], parts[0])\n",
    "\n",
    "  newsgroup_list = os.listdir(train_path)\n",
    "  newsgroup_list.sort()\n",
    "\n",
    "  train_data = collect_data_from(\n",
    "    parent_path = train_path,\n",
    "    newsgroup_list = newsgroup_list,\n",
    "    word_count = word_count\n",
    "  )\n",
    "\n",
    "  vocab = [word for word, freq in zip(word_count.keys(), word_count.values()) if freq > 10]\n",
    "  vocab.sort()\n",
    "\n",
    "  test_data = collect_data_from(\n",
    "    parent_path = test_path,\n",
    "    newsgroup_list= newsgroup_list\n",
    "  )\n",
    "\n",
    "  if not os.path.exists(path + 'w2v'):\n",
    "    os.makedirs(path + 'w2v')\n",
    "\n",
    "  with open(path + 'w2v\\\\vocab-raw.txt', 'w') as f:\n",
    "    f.write('\\n'.join(vocab))\n",
    "\n",
    "  with open(path + 'w2v\\\\20news-train-raws.txt', 'w') as f:\n",
    "    f.write('\\n'.join(train_data))\n",
    "\n",
    "  with open (path + 'w2v\\\\20news-test-raws.txt', 'w') as f:\n",
    "    f.write('\\n'.join(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing: 0-alt.atheism\n",
      "Processing: 1-comp.graphics\n",
      "Processing: 2-comp.os.ms-windows.misc\n",
      "Processing: 3-comp.sys.ibm.pc.hardware\n",
      "Processing: 4-comp.sys.mac.hardware\n",
      "Processing: 5-comp.windows.x\n",
      "Processing: 6-misc.forsale\n",
      "Processing: 7-rec.autos\n",
      "Processing: 8-rec.motorcycles\n",
      "Processing: 9-rec.sport.baseball\n",
      "Processing: 10-rec.sport.hockey\n",
      "Processing: 11-sci.crypt\n",
      "Processing: 12-sci.electronics\n",
      "Processing: 13-sci.med\n",
      "Processing: 14-sci.space\n",
      "Processing: 15-soc.religion.christian\n",
      "Processing: 16-talk.politics.guns\n",
      "Processing: 17-talk.politics.mideast\n",
      "Processing: 18-talk.politics.misc\n",
      "Processing: 19-talk.religion.misc\n",
      "Processing: 0-alt.atheism\n",
      "Processing: 1-comp.graphics\n",
      "Processing: 2-comp.os.ms-windows.misc\n",
      "Processing: 3-comp.sys.ibm.pc.hardware\n",
      "Processing: 4-comp.sys.mac.hardware\n",
      "Processing: 5-comp.windows.x\n",
      "Processing: 6-misc.forsale\n",
      "Processing: 7-rec.autos\n",
      "Processing: 8-rec.motorcycles\n",
      "Processing: 9-rec.sport.baseball\n",
      "Processing: 10-rec.sport.hockey\n",
      "Processing: 11-sci.crypt\n",
      "Processing: 12-sci.electronics\n",
      "Processing: 13-sci.med\n",
      "Processing: 14-sci.space\n",
      "Processing: 15-soc.religion.christian\n",
      "Processing: 16-talk.politics.guns\n",
      "Processing: 17-talk.politics.mideast\n",
      "Processing: 18-talk.politics.misc\n",
      "Processing: 19-talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "gen_data_and_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DOC_LENGTH = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(data_path, vocab_path):\n",
    "    unknown_ID = 1\n",
    "    padding_ID = 0\n",
    "    with open(vocab_path) as f:\n",
    "        vocab = dict([(word, word_id + 2) for word_id, word in enumerate(f.read().splitlines())])\n",
    "\n",
    "    with open(data_path) as f:\n",
    "        documents = [(line.split('<fff>')[0], line.split('<fff>')[1], line.split('<fff>')[2]) for line in f.read().splitlines()]\n",
    "    \n",
    "    encoded_data = []\n",
    "    \n",
    "    for document in documents:\n",
    "        label, doc_id, text = document\n",
    "        words = text.split()[:MAX_DOC_LENGTH]\n",
    "        sentence_length = len(words)\n",
    "\n",
    "        encoded_text = []\n",
    "        for word in words:\n",
    "            if word in vocab:\n",
    "                encoded_text.append(str(vocab[word]))\n",
    "            else:\n",
    "                encoded_text.append(str(unknown_ID))\n",
    "        \n",
    "        if sentence_length < MAX_DOC_LENGTH:\n",
    "            num_padding = MAX_DOC_LENGTH - sentence_length\n",
    "            for _ in range(num_padding):\n",
    "                encoded_text.append(str(padding_ID))\n",
    "\n",
    "        encoded_data.append(str(label) + '<fff>' + str(doc_id) + '<fff>' + str(sentence_length) + '<fff>' + ' '.join(encoded_text))\n",
    "\n",
    "    dir_name = '\\\\'.join(data_path.split('\\\\')[:-1])\n",
    "    file_name = '-'.join(data_path.split('\\\\')[-1].split('-')[:-1]) + '-encoded.txt'\n",
    "\n",
    "    with open(dir_name + '\\\\' + file_name, 'w') as f:\n",
    "        f.write('\\n'.join(encoded_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '..\\\\datasets\\\\20news-bydate\\\\w2v\\\\'\n",
    "vocab_path = '..\\\\datasets\\\\20news-bydate\\\\w2v\\\\vocab-raw.txt'\n",
    "encode_data(data_path = data_path + '20news-train-raws.txt', vocab_path = vocab_path)\n",
    "encode_data(data_path = data_path + '20news-test-raws.txt', vocab_path = vocab_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}