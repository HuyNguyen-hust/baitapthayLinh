{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "session-04_RNNs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZBw0kF21nR-"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nb-XeJv25bZ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca8107df-dda0-4dcf-860d-8490de4d68b9"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUfyPu7x6Xmt"
      },
      "source": [
        "MAX_DOC_LENGTH = 500\n",
        "NUM_CLASSES = 20\n",
        "path = '\\\\20news-bydate\\\\'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAKA-aGn2H8H"
      },
      "source": [
        "class RNN:\n",
        "  def __init__(self, vocab_size, embedding_size, lstm_size, batch_size):\n",
        "    self._vocab_size = vocab_size\n",
        "    self._embedding_size = embedding_size\n",
        "    self._lstm_size = lstm_size\n",
        "    self._batch_size = batch_size\n",
        "\n",
        "    self._data = tf.placeholder(tf.int32, shape = [batch_size, MAX_DOC_LENGTH])\n",
        "    self._labels = tf.placeholder(tf.int32, shape = [batch_size, ])\n",
        "    self._sentence_lengths = tf.placeholder(tf.int32, shape = [batch_size, ])\n",
        "    self._final_tokens = tf.placeholder(tf.int32, shape = [batch_size, ])\n",
        "\n",
        "  def embedding_layer(self, indices):\n",
        "    pretrained_vectors = []\n",
        "    pretrained_vectors.append(np.zeros(self._embedding_size))\n",
        "    np.random.seed(2021)\n",
        "\n",
        "    for _ in range(self._vocab_size + 1):\n",
        "      pretrained_vectors.append(np.random.normal(loc = 0., scale = 1., size = self._embedding_size))\n",
        "\n",
        "    pretrained_vectors = np.array(pretrained_vectors)\n",
        "\n",
        "    with tf.variable_scope('embedding', reuse = tf.AUTO_REUSE):\n",
        "      self._embedding_matrix = tf.get_variable(\n",
        "          name = 'embedding',\n",
        "          shape = (self._vocab_size + 2, self._embedding_size),\n",
        "          initializer = tf.constant_initializer(pretrained_vectors)\n",
        "      )\n",
        "\n",
        "    return tf.nn.embedding_lookup(self._embedding_matrix, indices)\n",
        "\n",
        "  def LSTM_layer(self, embeddings):\n",
        "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(self._lstm_size)\n",
        "    zero_state = tf.zeros(shape = (self._batch_size, self._lstm_size))\n",
        "    initial_state = tf.contrib.rnn.LSTMStateTuple(zero_state, zero_state)\n",
        "\n",
        "    lstm_inputs = tf.unstack(tf.transpose(embeddings, perm = [1, 0, 2]))\n",
        "    with tf.variable_scope('lstm', reuse = tf.AUTO_REUSE):\n",
        "      lstm_outputs, last_state = tf.nn.static_rnn (\n",
        "          cell = lstm_cell,\n",
        "          inputs = lstm_inputs,\n",
        "          initial_state = initial_state,\n",
        "          sequence_length = self._sentence_lengths\n",
        "      )\n",
        "\n",
        "    lstm_outputs = tf.unstack(tf.transpose(lstm_outputs, perm = [1, 0, 2]))\n",
        "    lstm_outputs = tf.concat(lstm_outputs, axis = 0)\n",
        "\n",
        "    mask = tf.sequence_mask(\n",
        "        lengths = self._sentence_lengths,\n",
        "        maxlen = MAX_DOC_LENGTH,\n",
        "        dtype = tf.float32\n",
        "    )\n",
        "\n",
        "    mask = tf.concat(tf.unstack(mask, axis = 0), axis = 0)\n",
        "    mask = tf.expand_dims(mask, -1)\n",
        "\n",
        "    lstm_outputs = mask * lstm_outputs\n",
        "    lstm_outputs_split = tf.split(lstm_outputs, num_or_size_splits = self._batch_size)\n",
        "    lstm_outputs_sum = tf.reduce_sum(lstm_outputs_split, axis = 1)\n",
        "    lstm_outputs_average = lstm_outputs_sum / tf.expand_dims(tf.cast(self._sentence_lengths, tf.float32), -1)\n",
        "\n",
        "    return lstm_outputs_average\n",
        "\n",
        "  def build_graph(self):\n",
        "    embeddings = self.embedding_layer(self._data)\n",
        "    lstm_outputs = self.LSTM_layer(embeddings)\n",
        "\n",
        "    with tf.variable_scope('final_layer_weights', reuse = tf.AUTO_REUSE):\n",
        "      weights = tf.get_variable(\n",
        "          name = 'final_layer_weights',\n",
        "          shape = (self._lstm_size, NUM_CLASSES),\n",
        "          initializer = tf.random_normal_initializer(seed = 2021)\n",
        "      )\n",
        "\n",
        "    with tf.variable_scope('final_layer_biases', reuse = tf.AUTO_REUSE):\n",
        "      biases = tf.get_variable(\n",
        "          name = 'final_layer_biases',\n",
        "          shape = (NUM_CLASSES),\n",
        "          initializer = tf.random_normal_initializer(seed = 2021)\n",
        "      )\n",
        "\n",
        "    logits = tf.matmul(lstm_outputs, weights) + biases\n",
        "\n",
        "    labels_one_hot = tf.one_hot(\n",
        "        indices = self._labels,\n",
        "        depth = NUM_CLASSES,\n",
        "        dtype = tf.float32\n",
        "    )\n",
        "\n",
        "    loss = tf.nn.softmax_cross_entropy_with_logits(\n",
        "        labels = labels_one_hot,\n",
        "        logits = logits\n",
        "    )\n",
        "\n",
        "    loss = tf.reduce_mean(loss)\n",
        "\n",
        "    probs = tf.nn.softmax(logits)\n",
        "    predicted_labels = tf.argmax(probs, axis = 1)\n",
        "    predicted_labels = tf.squeeze(predicted_labels)\n",
        "\n",
        "    return predicted_labels, loss\n",
        "\n",
        "  def trainer(self, loss, learning_rate):\n",
        "    with tf.variable_scope('optimizer', reuse = tf.AUTO_REUSE):\n",
        "      train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
        "      return train_op"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg2RzJ2hzbm4"
      },
      "source": [
        "class DataReader:\n",
        "  def __init__(self, path, batch_size):\n",
        "    self._batch_size = batch_size\n",
        "    with open(path, encoding = 'ISO-8859-1') as f:\n",
        "      d_lines = f.read().splitlines()\n",
        "    self._data = []\n",
        "    self._labels = []\n",
        "    self._sentence_lengths = []\n",
        "    self._final_tokens = []\n",
        "    \n",
        "    for data_id, line in enumerate(d_lines):\n",
        "      features = line.split('<fff>')\n",
        "      label, doc_id, sentence_length = int(features[0]), int(features[1]), int(features[2])\n",
        "      tokens = [int(token) for token in features[3].split()]\n",
        "      final_token = tokens[sentence_length - 1]\n",
        "      self._data.append(tokens)\n",
        "      self._labels.append(label)\n",
        "      self._sentence_lengths.append(sentence_length)\n",
        "      self._final_tokens.append(final_token)\n",
        "    \n",
        "    self._data = np.array(self._data)\n",
        "    self._labels = np.array(self._labels)\n",
        "    self._sentence_lengths = np.array(self._sentence_lengths)\n",
        "    self._final_tokens = np.array(self._final_tokens)\n",
        "\n",
        "    self._num_epoch = 0\n",
        "    self._current_part = 0\n",
        "  \n",
        "  def next_batch(self):\n",
        "    start = self._current_part * self._batch_size\n",
        "    end = start + self._batch_size\n",
        "    self._current_part += 1\n",
        "\n",
        "    if end + self._batch_size > len(self._data):\n",
        "      self._num_epoch += 1\n",
        "      self._current_part = 0\n",
        "      indices = range(len(self._data))\n",
        "      random.seed(2021)\n",
        "      random.shuffle(list(indices))\n",
        "      tmp_data = []\n",
        "      tmp_labels = []\n",
        "      tmp_sentence_lengths = []\n",
        "      tmp_final_tokens = []\n",
        "      for idx in indices:\n",
        "        tmp_data.append(self._data[idx])\n",
        "        tmp_labels.append(self._labels[idx])\n",
        "        tmp_sentence_lengths.append(self._sentence_lengths[idx])\n",
        "        tmp_final_tokens.append(self._final_tokens[idx])\n",
        "      self._data, self._labels, self._sentence_lengths, self._final_tokens = tmp_data, tmp_labels, tmp_sentence_lengths, tmp_final_tokens\n",
        "    \n",
        "    return self._data[start:end], self._labels[start:end], self._sentence_lengths[start:end], self._final_tokens[start:end]\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuPeF8u6zWsN"
      },
      "source": [
        "def load_dataset():\n",
        "  train_data_reader = DataReader(\n",
        "      path = path + 'w2v\\\\20news-train-encoded.txt',\n",
        "      batch_size = 50\n",
        "  )\n",
        "  test_data_reader = DataReader(\n",
        "      path = path + 'w2v\\\\20news-test-encoded.txt',\n",
        "      batch_size = 50\n",
        "  )\n",
        "  \n",
        "  return train_data_reader, test_data_reader\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPNfadXc_Nrz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f1b2e60-084a-4d7f-a7f3-42e6cdb9df8d"
      },
      "source": [
        "with open(path + 'w2v\\\\vocab-raw.txt', encoding = 'ISO-8859-1') as f:\n",
        "  vocab_size = len(f.read().splitlines())\n",
        "\n",
        "  tf.random.set_random_seed(2021)\n",
        "  rnn = RNN(\n",
        "      vocab_size = vocab_size,\n",
        "      embedding_size = 300,\n",
        "      lstm_size = 50,\n",
        "      batch_size = 50\n",
        "  )\n",
        "  predicted_labels, loss = rnn.build_graph()\n",
        "  train_op = rnn.trainer(loss = loss, learning_rate = 0.01)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-4-ef1960e9e461>:33: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-4-ef1960e9e461>:43: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From <ipython-input-4-ef1960e9e461>:93: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XRisgjVqJlp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d0a0127-6ee4-419c-dc9f-25702d74aa2f"
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  train_data_reader, test_data_reader = load_dataset()\n",
        "\n",
        "  step = 0\n",
        "  MAX_STEP = 11000\n",
        "\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  while step < MAX_STEP:\n",
        "    next_train_batch = train_data_reader.next_batch()\n",
        "    train_data, train_labels, train_sentence_lengths, train_final_tokens = next_train_batch\n",
        "    plabels_eval, loss_eval, _ = sess.run(\n",
        "        [predicted_labels, loss, train_op],\n",
        "        feed_dict = {\n",
        "            rnn._data: train_data,\n",
        "            rnn._labels: train_labels,\n",
        "            rnn._sentence_lengths: train_sentence_lengths,\n",
        "            rnn._final_tokens: train_final_tokens\n",
        "        }\n",
        "    )\n",
        "    step += 1\n",
        "    if step % 20 == 0:\n",
        "      print('step: ' + str(step) +' - loss: ', str(loss_eval))\n",
        "    if train_data_reader._current_part == 0:\n",
        "      num_true_preds = 0\n",
        "      while True:\n",
        "        next_test_batch = test_data_reader.next_batch()\n",
        "        test_data, test_labels, test_sentence_lenghts, test_final_tokens = next_test_batch\n",
        "\n",
        "        test_plabels_eval = sess.run(\n",
        "            predicted_labels,\n",
        "            feed_dict = {\n",
        "                rnn._data: test_data,\n",
        "                rnn._labels: test_labels,\n",
        "                rnn._sentence_lengths: test_sentence_lenghts,\n",
        "                rnn._final_tokens: test_final_tokens\n",
        "            }\n",
        "        )\n",
        "        matches = np.equal(test_plabels_eval, test_labels)\n",
        "        num_true_preds += np.sum(matches.astype(float))\n",
        "\n",
        "        if test_data_reader._current_part == 0:\n",
        "          break\n",
        "      \n",
        "      print('Epoch: ', train_data_reader._num_epoch)\n",
        "      print('Accuracy on test data: ', num_true_preds * 100. / len(test_data_reader._data))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step: 20 - loss:  0.0012361328\n",
            "step: 40 - loss:  0.4900295\n",
            "step: 60 - loss:  5.7572503\n",
            "step: 80 - loss:  0.86787254\n",
            "step: 100 - loss:  3.8929074\n",
            "step: 120 - loss:  5.416351\n",
            "step: 140 - loss:  3.0255537\n",
            "step: 160 - loss:  3.612365\n",
            "step: 180 - loss:  3.6553693\n",
            "step: 200 - loss:  9.0240965\n",
            "step: 220 - loss:  3.5073512\n",
            "Epoch:  1\n",
            "Accuracy on test data:  4.84599044078598\n",
            "step: 240 - loss:  3.8009467\n",
            "step: 260 - loss:  2.95274\n",
            "step: 280 - loss:  2.6134386\n",
            "step: 300 - loss:  2.9435427\n",
            "step: 320 - loss:  3.3065193\n",
            "step: 340 - loss:  1.9001852\n",
            "step: 360 - loss:  3.9714172\n",
            "step: 380 - loss:  2.4186883\n",
            "step: 400 - loss:  1.2801465\n",
            "step: 420 - loss:  2.5218585\n",
            "step: 440 - loss:  2.4225063\n",
            "Epoch:  2\n",
            "Accuracy on test data:  9.373340414232608\n",
            "step: 460 - loss:  2.1379087\n",
            "step: 480 - loss:  2.3613937\n",
            "step: 500 - loss:  2.5775368\n",
            "step: 520 - loss:  1.3846714\n",
            "step: 540 - loss:  2.2108655\n",
            "step: 560 - loss:  2.1179504\n",
            "step: 580 - loss:  1.4321709\n",
            "step: 600 - loss:  2.5070975\n",
            "step: 620 - loss:  3.0569947\n",
            "step: 640 - loss:  2.431113\n",
            "step: 660 - loss:  0.60945266\n",
            "Epoch:  3\n",
            "Accuracy on test data:  18.388210302708444\n",
            "step: 680 - loss:  1.9047458\n",
            "step: 700 - loss:  2.2182832\n",
            "step: 720 - loss:  1.9393555\n",
            "step: 740 - loss:  1.4402475\n",
            "step: 760 - loss:  2.3466651\n",
            "step: 780 - loss:  1.148819\n",
            "step: 800 - loss:  0.98841375\n",
            "step: 820 - loss:  2.4217706\n",
            "step: 840 - loss:  0.9331268\n",
            "step: 860 - loss:  1.4119862\n",
            "step: 880 - loss:  0.9807245\n",
            "step: 900 - loss:  2.0550988\n",
            "Epoch:  4\n",
            "Accuracy on test data:  44.31757833244822\n",
            "step: 920 - loss:  1.7371063\n",
            "step: 940 - loss:  1.9588537\n",
            "step: 960 - loss:  1.7381351\n",
            "step: 980 - loss:  0.6566563\n",
            "step: 1000 - loss:  1.3083988\n",
            "step: 1020 - loss:  0.47770935\n",
            "step: 1040 - loss:  0.5631681\n",
            "step: 1060 - loss:  1.8539369\n",
            "step: 1080 - loss:  1.4249951\n",
            "step: 1100 - loss:  0.9390311\n",
            "step: 1120 - loss:  0.72150797\n",
            "Epoch:  5\n",
            "Accuracy on test data:  59.93096123207648\n",
            "step: 1140 - loss:  0.96022123\n",
            "step: 1160 - loss:  0.8683756\n",
            "step: 1180 - loss:  1.4697201\n",
            "step: 1200 - loss:  0.9306688\n",
            "step: 1220 - loss:  0.6153358\n",
            "step: 1240 - loss:  0.24671677\n",
            "step: 1260 - loss:  0.18776026\n",
            "step: 1280 - loss:  0.35253862\n",
            "step: 1300 - loss:  0.1963931\n",
            "step: 1320 - loss:  0.50299567\n",
            "step: 1340 - loss:  0.55428874\n",
            "Epoch:  6\n",
            "Accuracy on test data:  69.4105151354222\n",
            "step: 1360 - loss:  0.5475818\n",
            "step: 1380 - loss:  0.7410212\n",
            "step: 1400 - loss:  0.69471437\n",
            "step: 1420 - loss:  0.27799386\n",
            "step: 1440 - loss:  0.46099433\n",
            "step: 1460 - loss:  0.21262817\n",
            "step: 1480 - loss:  0.062316805\n",
            "step: 1500 - loss:  0.4363798\n",
            "step: 1520 - loss:  0.17218544\n",
            "step: 1540 - loss:  0.4590333\n",
            "step: 1560 - loss:  0.11368904\n",
            "step: 1580 - loss:  0.4260193\n",
            "Epoch:  7\n",
            "Accuracy on test data:  71.93308550185874\n",
            "step: 1600 - loss:  0.28984153\n",
            "step: 1620 - loss:  0.40246338\n",
            "step: 1640 - loss:  0.27992284\n",
            "step: 1660 - loss:  0.1847077\n",
            "step: 1680 - loss:  0.08806467\n",
            "step: 1700 - loss:  0.13279513\n",
            "step: 1720 - loss:  0.12021466\n",
            "step: 1740 - loss:  0.14839113\n",
            "step: 1760 - loss:  0.102956735\n",
            "step: 1780 - loss:  0.10104649\n",
            "step: 1800 - loss:  0.03792705\n",
            "Epoch:  8\n",
            "Accuracy on test data:  74.84067976633033\n",
            "step: 1820 - loss:  0.29117274\n",
            "step: 1840 - loss:  0.1498822\n",
            "step: 1860 - loss:  0.20011438\n",
            "step: 1880 - loss:  0.079230994\n",
            "step: 1900 - loss:  0.07565904\n",
            "step: 1920 - loss:  0.04078603\n",
            "step: 1940 - loss:  0.055014186\n",
            "step: 1960 - loss:  0.060143426\n",
            "step: 1980 - loss:  0.02215125\n",
            "step: 2000 - loss:  0.049137726\n",
            "step: 2020 - loss:  0.11358602\n",
            "Epoch:  9\n",
            "Accuracy on test data:  75.34519383961764\n",
            "step: 2040 - loss:  0.053465944\n",
            "step: 2060 - loss:  0.10323217\n",
            "step: 2080 - loss:  0.18345517\n",
            "step: 2100 - loss:  0.05081636\n",
            "step: 2120 - loss:  0.07010848\n",
            "step: 2140 - loss:  0.023900745\n",
            "step: 2160 - loss:  0.020417042\n",
            "step: 2180 - loss:  0.051532168\n",
            "step: 2200 - loss:  0.018248625\n",
            "step: 2220 - loss:  0.055956107\n",
            "step: 2240 - loss:  0.012149525\n",
            "step: 2260 - loss:  0.07161038\n",
            "Epoch:  10\n",
            "Accuracy on test data:  75.50451407328731\n",
            "step: 2280 - loss:  0.042687096\n",
            "step: 2300 - loss:  0.061035786\n",
            "step: 2320 - loss:  0.025368305\n",
            "step: 2340 - loss:  0.03173483\n",
            "step: 2360 - loss:  0.015966339\n",
            "step: 2380 - loss:  0.01328068\n",
            "step: 2400 - loss:  0.032300614\n",
            "step: 2420 - loss:  0.022763096\n",
            "step: 2440 - loss:  0.037461314\n",
            "step: 2460 - loss:  0.010091045\n",
            "step: 2480 - loss:  0.061256457\n",
            "Epoch:  11\n",
            "Accuracy on test data:  76.0355815188529\n",
            "step: 2500 - loss:  0.026437147\n",
            "step: 2520 - loss:  0.087495565\n",
            "step: 2540 - loss:  0.03109972\n",
            "step: 2560 - loss:  0.022998458\n",
            "step: 2580 - loss:  0.01792081\n",
            "step: 2600 - loss:  0.008700429\n",
            "step: 2620 - loss:  0.007904387\n",
            "step: 2640 - loss:  0.008159689\n",
            "step: 2660 - loss:  0.0051422953\n",
            "step: 2680 - loss:  0.0110412575\n",
            "step: 2700 - loss:  0.021227537\n",
            "Epoch:  12\n",
            "Accuracy on test data:  76.10196494954859\n",
            "step: 2720 - loss:  0.024676073\n",
            "step: 2740 - loss:  0.038321722\n",
            "step: 2760 - loss:  0.02760063\n",
            "step: 2780 - loss:  0.020653844\n",
            "step: 2800 - loss:  0.009224682\n",
            "step: 2820 - loss:  0.0041851713\n",
            "step: 2840 - loss:  0.005461006\n",
            "step: 2860 - loss:  0.011980969\n",
            "step: 2880 - loss:  0.007878934\n",
            "step: 2900 - loss:  0.010731361\n",
            "step: 2920 - loss:  0.0020391985\n",
            "Epoch:  13\n",
            "Accuracy on test data:  76.2081784386617\n",
            "step: 2940 - loss:  0.012882054\n",
            "step: 2960 - loss:  0.044432316\n",
            "step: 2980 - loss:  0.016817257\n",
            "step: 3000 - loss:  0.020922167\n",
            "step: 3020 - loss:  0.005116474\n",
            "step: 3040 - loss:  0.009555459\n",
            "step: 3060 - loss:  0.00250665\n",
            "step: 3080 - loss:  0.03597267\n",
            "step: 3100 - loss:  0.00803601\n",
            "step: 3120 - loss:  0.016207244\n",
            "step: 3140 - loss:  0.0066909683\n",
            "step: 3160 - loss:  0.015792716\n",
            "Epoch:  14\n",
            "Accuracy on test data:  76.08868826340945\n",
            "step: 3180 - loss:  0.013061205\n",
            "step: 3200 - loss:  0.017313832\n",
            "step: 3220 - loss:  0.015673073\n",
            "step: 3240 - loss:  0.016560253\n",
            "step: 3260 - loss:  0.008275059\n",
            "step: 3280 - loss:  0.0028331252\n",
            "step: 3300 - loss:  0.005942843\n",
            "step: 3320 - loss:  0.007582217\n",
            "step: 3340 - loss:  0.0070299488\n",
            "step: 3360 - loss:  0.0059605394\n",
            "step: 3380 - loss:  0.005490743\n",
            "Epoch:  15\n",
            "Accuracy on test data:  76.15507169410515\n",
            "step: 3400 - loss:  0.021297488\n",
            "step: 3420 - loss:  0.010379547\n",
            "step: 3440 - loss:  0.010210676\n",
            "step: 3460 - loss:  0.0063671367\n",
            "step: 3480 - loss:  0.0041375966\n",
            "step: 3500 - loss:  0.002238256\n",
            "step: 3520 - loss:  0.0011762966\n",
            "step: 3540 - loss:  0.005092765\n",
            "step: 3560 - loss:  0.0020401624\n",
            "step: 3580 - loss:  0.0042067794\n",
            "step: 3600 - loss:  0.0055228984\n",
            "Epoch:  16\n",
            "Accuracy on test data:  76.42060541688795\n",
            "step: 3620 - loss:  0.008172044\n",
            "step: 3640 - loss:  0.0077705234\n",
            "step: 3660 - loss:  0.019851085\n",
            "step: 3680 - loss:  0.003962916\n",
            "step: 3700 - loss:  0.0041334652\n",
            "step: 3720 - loss:  0.0023481029\n",
            "step: 3740 - loss:  0.0012340313\n",
            "step: 3760 - loss:  0.003125987\n",
            "step: 3780 - loss:  0.002841911\n",
            "step: 3800 - loss:  0.006399244\n",
            "step: 3820 - loss:  0.002644906\n",
            "step: 3840 - loss:  0.0068217884\n",
            "Epoch:  17\n",
            "Accuracy on test data:  76.06213489113118\n",
            "step: 3860 - loss:  0.005952945\n",
            "step: 3880 - loss:  0.0052237553\n",
            "step: 3900 - loss:  0.0057555307\n",
            "step: 3920 - loss:  0.003980277\n",
            "step: 3940 - loss:  0.0016756046\n",
            "step: 3960 - loss:  0.0021142075\n",
            "step: 3980 - loss:  0.0030927265\n",
            "step: 4000 - loss:  0.003412479\n",
            "step: 4020 - loss:  0.002406888\n",
            "step: 4040 - loss:  0.0031984544\n",
            "step: 4060 - loss:  0.0013961893\n",
            "Epoch:  18\n",
            "Accuracy on test data:  76.22145512480085\n",
            "step: 4080 - loss:  0.005928068\n",
            "step: 4100 - loss:  0.0036305063\n",
            "step: 4120 - loss:  0.005606358\n",
            "step: 4140 - loss:  0.0026029963\n",
            "step: 4160 - loss:  0.002694971\n",
            "step: 4180 - loss:  0.0015875578\n",
            "step: 4200 - loss:  0.0016199122\n",
            "step: 4220 - loss:  0.0019366738\n",
            "step: 4240 - loss:  0.0008832442\n",
            "step: 4260 - loss:  0.001445263\n",
            "step: 4280 - loss:  0.003820625\n",
            "Epoch:  19\n",
            "Accuracy on test data:  76.32766861391397\n",
            "step: 4300 - loss:  0.0030563262\n",
            "step: 4320 - loss:  0.003431447\n",
            "step: 4340 - loss:  0.004640404\n",
            "step: 4360 - loss:  0.0020120388\n",
            "step: 4380 - loss:  0.0024250872\n",
            "step: 4400 - loss:  0.00074710255\n",
            "step: 4420 - loss:  0.0008306778\n",
            "step: 4440 - loss:  0.001966144\n",
            "step: 4460 - loss:  0.0009772138\n",
            "step: 4480 - loss:  0.0020368374\n",
            "step: 4500 - loss:  0.0009949196\n",
            "step: 4520 - loss:  0.0033524614\n",
            "Epoch:  20\n",
            "Accuracy on test data:  76.38077535847053\n",
            "step: 4540 - loss:  0.0018817863\n",
            "step: 4560 - loss:  0.0025032854\n",
            "step: 4580 - loss:  0.001474662\n",
            "step: 4600 - loss:  0.002427892\n",
            "step: 4620 - loss:  0.00097224204\n",
            "step: 4640 - loss:  0.0006940554\n",
            "step: 4660 - loss:  0.0016709826\n",
            "step: 4680 - loss:  0.0013164352\n",
            "step: 4700 - loss:  0.0014119771\n",
            "step: 4720 - loss:  0.00083454134\n",
            "step: 4740 - loss:  0.0036576167\n",
            "Epoch:  21\n",
            "Accuracy on test data:  76.32766861391397\n",
            "step: 4760 - loss:  0.0018378533\n",
            "step: 4780 - loss:  0.0048995786\n",
            "step: 4800 - loss:  0.0016892529\n",
            "step: 4820 - loss:  0.0011561268\n",
            "step: 4840 - loss:  0.0010484771\n",
            "step: 4860 - loss:  0.0006853541\n",
            "step: 4880 - loss:  0.00051626033\n",
            "step: 4900 - loss:  0.0006396396\n",
            "step: 4920 - loss:  0.00038557776\n",
            "step: 4940 - loss:  0.0007825291\n",
            "step: 4960 - loss:  0.0011358842\n",
            "Epoch:  22\n",
            "Accuracy on test data:  76.38077535847053\n",
            "step: 4980 - loss:  0.0017198937\n",
            "step: 5000 - loss:  0.0019904054\n",
            "step: 5020 - loss:  0.0019070858\n",
            "step: 5040 - loss:  0.001301272\n",
            "step: 5060 - loss:  0.00068335026\n",
            "step: 5080 - loss:  0.00031030737\n",
            "step: 5100 - loss:  0.0005858093\n",
            "step: 5120 - loss:  0.0008922605\n",
            "step: 5140 - loss:  0.0006175019\n",
            "step: 5160 - loss:  0.00090142986\n",
            "step: 5180 - loss:  0.00016731035\n",
            "Epoch:  23\n",
            "Accuracy on test data:  76.56664896441848\n",
            "step: 5200 - loss:  0.0011311489\n",
            "step: 5220 - loss:  0.025135124\n",
            "step: 5240 - loss:  0.0012354553\n",
            "step: 5260 - loss:  0.008549499\n",
            "step: 5280 - loss:  0.0005804245\n",
            "step: 5300 - loss:  0.00076801615\n",
            "step: 5320 - loss:  0.00023239758\n",
            "step: 5340 - loss:  0.011374274\n",
            "step: 5360 - loss:  0.00059349544\n",
            "step: 5380 - loss:  0.001221643\n",
            "step: 5400 - loss:  0.0005189827\n",
            "step: 5420 - loss:  0.001282042\n",
            "Epoch:  24\n",
            "Accuracy on test data:  76.63303239511418\n",
            "step: 5440 - loss:  0.0012810028\n",
            "step: 5460 - loss:  0.0013410057\n",
            "step: 5480 - loss:  0.001503862\n",
            "step: 5500 - loss:  0.00561966\n",
            "step: 5520 - loss:  0.0006614348\n",
            "step: 5540 - loss:  0.00021219294\n",
            "step: 5560 - loss:  0.00055700506\n",
            "step: 5580 - loss:  0.00061565265\n",
            "step: 5600 - loss:  0.0005394105\n",
            "step: 5620 - loss:  0.00056566787\n",
            "step: 5640 - loss:  0.0005040383\n",
            "Epoch:  25\n",
            "Accuracy on test data:  76.7392458842273\n",
            "step: 5660 - loss:  0.009150691\n",
            "step: 5680 - loss:  0.0009233797\n",
            "step: 5700 - loss:  0.0008403533\n",
            "step: 5720 - loss:  0.00063493056\n",
            "step: 5740 - loss:  0.00041277826\n",
            "step: 5760 - loss:  0.00030833718\n",
            "step: 5780 - loss:  0.00013664803\n",
            "step: 5800 - loss:  0.00066152913\n",
            "step: 5820 - loss:  0.00012566402\n",
            "step: 5840 - loss:  0.00048085785\n",
            "step: 5860 - loss:  0.0005807026\n",
            "Epoch:  26\n",
            "Accuracy on test data:  76.69941582580988\n",
            "step: 5880 - loss:  0.0010170275\n",
            "step: 5900 - loss:  0.0008279349\n",
            "step: 5920 - loss:  0.004929676\n",
            "step: 5940 - loss:  0.00048123166\n",
            "step: 5960 - loss:  0.00039686143\n",
            "step: 5980 - loss:  0.0003836442\n",
            "step: 6000 - loss:  0.00016935603\n",
            "step: 6020 - loss:  0.00048293045\n",
            "step: 6040 - loss:  0.0004303517\n",
            "step: 6060 - loss:  0.00066210894\n",
            "step: 6080 - loss:  0.00031609577\n",
            "step: 6100 - loss:  0.00077218906\n",
            "Epoch:  27\n",
            "Accuracy on test data:  76.6728624535316\n",
            "step: 6120 - loss:  0.00087479985\n",
            "step: 6140 - loss:  0.00069043634\n",
            "step: 6160 - loss:  0.00078102946\n",
            "step: 6180 - loss:  0.0005404766\n",
            "step: 6200 - loss:  0.00019036092\n",
            "step: 6220 - loss:  0.00031542458\n",
            "step: 6240 - loss:  0.0003407234\n",
            "step: 6260 - loss:  0.00040775098\n",
            "step: 6280 - loss:  0.0003032514\n",
            "step: 6300 - loss:  0.00042851278\n",
            "step: 6320 - loss:  0.00020093292\n",
            "Epoch:  28\n",
            "Accuracy on test data:  76.92511949017525\n",
            "step: 6340 - loss:  0.0007366736\n",
            "step: 6360 - loss:  0.00051907287\n",
            "step: 6380 - loss:  0.0006928465\n",
            "step: 6400 - loss:  0.00035112686\n",
            "step: 6420 - loss:  0.00035498574\n",
            "step: 6440 - loss:  0.00018970581\n",
            "step: 6460 - loss:  0.00021743197\n",
            "step: 6480 - loss:  0.0002926041\n",
            "step: 6500 - loss:  0.00015502598\n",
            "step: 6520 - loss:  0.00021767132\n",
            "step: 6540 - loss:  0.0005051456\n",
            "Epoch:  29\n",
            "Accuracy on test data:  76.85873605947955\n",
            "step: 6560 - loss:  0.000433997\n",
            "step: 6580 - loss:  0.0005285025\n",
            "step: 6600 - loss:  0.00075205276\n",
            "step: 6620 - loss:  0.00028155703\n",
            "step: 6640 - loss:  0.00034605994\n",
            "step: 6660 - loss:  0.00017382715\n",
            "step: 6680 - loss:  0.00019133357\n",
            "step: 6700 - loss:  0.00043783148\n",
            "step: 6720 - loss:  0.00023431962\n",
            "step: 6740 - loss:  0.0003688761\n",
            "step: 6760 - loss:  0.00013500359\n",
            "step: 6780 - loss:  0.000753358\n",
            "Epoch:  30\n",
            "Accuracy on test data:  76.57992565055763\n",
            "step: 6800 - loss:  0.00039416866\n",
            "step: 6820 - loss:  0.00067403226\n",
            "step: 6840 - loss:  0.00045725887\n",
            "step: 6860 - loss:  0.0005226776\n",
            "step: 6880 - loss:  0.00026444515\n",
            "step: 6900 - loss:  0.00013343937\n",
            "step: 6920 - loss:  0.00053943304\n",
            "step: 6940 - loss:  0.00040190146\n",
            "step: 6960 - loss:  0.00035048963\n",
            "step: 6980 - loss:  0.00015305217\n",
            "step: 7000 - loss:  0.0009991511\n",
            "Epoch:  31\n",
            "Accuracy on test data:  76.69941582580988\n",
            "step: 7020 - loss:  0.0006570008\n",
            "step: 7040 - loss:  0.0014419204\n",
            "step: 7060 - loss:  0.0005450662\n",
            "step: 7080 - loss:  0.0004005464\n",
            "step: 7100 - loss:  0.0009916023\n",
            "step: 7120 - loss:  0.0002660932\n",
            "step: 7140 - loss:  0.00024192409\n",
            "step: 7160 - loss:  0.0004792057\n",
            "step: 7180 - loss:  0.0002154585\n",
            "step: 7200 - loss:  0.0005465435\n",
            "step: 7220 - loss:  0.0020630744\n",
            "Epoch:  32\n",
            "Accuracy on test data:  75.84970791290493\n",
            "step: 7240 - loss:  0.0011799766\n",
            "step: 7260 - loss:  0.0013244175\n",
            "step: 7280 - loss:  0.0012970916\n",
            "step: 7300 - loss:  0.0002714335\n",
            "step: 7320 - loss:  0.00047688\n",
            "step: 7340 - loss:  0.00095105515\n",
            "step: 7360 - loss:  0.0016383919\n",
            "step: 7380 - loss:  1.1361147e-05\n",
            "step: 7400 - loss:  1.00486495e-05\n",
            "step: 7420 - loss:  8.229105\n",
            "step: 7440 - loss:  2.7337619e-05\n",
            "Epoch:  33\n",
            "Accuracy on test data:  10.07700477960701\n",
            "step: 7460 - loss:  3.5612762\n",
            "step: 7480 - loss:  2.9750373\n",
            "step: 7500 - loss:  0.105723284\n",
            "step: 7520 - loss:  0.5521572\n",
            "step: 7540 - loss:  1.5295187\n",
            "step: 7560 - loss:  0.11699948\n",
            "step: 7580 - loss:  0.41084945\n",
            "step: 7600 - loss:  2.3312025\n",
            "step: 7620 - loss:  0.4314102\n",
            "step: 7640 - loss:  0.3789794\n",
            "step: 7660 - loss:  0.50373816\n",
            "step: 7680 - loss:  1.1585268\n",
            "Epoch:  34\n",
            "Accuracy on test data:  52.52257036643654\n",
            "step: 7700 - loss:  0.5843528\n",
            "step: 7720 - loss:  1.6455597\n",
            "step: 7740 - loss:  0.8290778\n",
            "step: 7760 - loss:  0.5844933\n",
            "step: 7780 - loss:  0.64217293\n",
            "step: 7800 - loss:  0.1512507\n",
            "step: 7820 - loss:  0.22062123\n",
            "step: 7840 - loss:  0.4079508\n",
            "step: 7860 - loss:  0.25698748\n",
            "step: 7880 - loss:  0.3153931\n",
            "step: 7900 - loss:  0.39956498\n",
            "Epoch:  35\n",
            "Accuracy on test data:  66.25066383430696\n",
            "step: 7920 - loss:  0.39473698\n",
            "step: 7940 - loss:  0.41000348\n",
            "step: 7960 - loss:  0.44101867\n",
            "step: 7980 - loss:  0.20367621\n",
            "step: 8000 - loss:  0.1783847\n",
            "step: 8020 - loss:  0.07007067\n",
            "step: 8040 - loss:  0.09416065\n",
            "step: 8060 - loss:  0.27397943\n",
            "step: 8080 - loss:  0.054570578\n",
            "step: 8100 - loss:  0.047177304\n",
            "step: 8120 - loss:  0.115380876\n",
            "Epoch:  36\n",
            "Accuracy on test data:  72.14551248008497\n",
            "step: 8140 - loss:  0.081992\n",
            "step: 8160 - loss:  0.19647373\n",
            "step: 8180 - loss:  0.34433922\n",
            "step: 8200 - loss:  0.07324325\n",
            "step: 8220 - loss:  0.09414442\n",
            "step: 8240 - loss:  0.024917554\n",
            "step: 8260 - loss:  0.016379299\n",
            "step: 8280 - loss:  0.03926643\n",
            "step: 8300 - loss:  0.03029475\n",
            "step: 8320 - loss:  0.07290784\n",
            "step: 8340 - loss:  0.0491468\n",
            "step: 8360 - loss:  0.059913225\n",
            "Epoch:  37\n",
            "Accuracy on test data:  73.10143388210302\n",
            "step: 8380 - loss:  0.095363334\n",
            "step: 8400 - loss:  0.07321127\n",
            "step: 8420 - loss:  0.042561628\n",
            "step: 8440 - loss:  0.07509808\n",
            "step: 8460 - loss:  0.018163068\n",
            "step: 8480 - loss:  0.031627584\n",
            "step: 8500 - loss:  0.018413052\n",
            "step: 8520 - loss:  0.098971024\n",
            "step: 8540 - loss:  0.032016803\n",
            "step: 8560 - loss:  0.053322792\n",
            "step: 8580 - loss:  0.024597941\n",
            "Epoch:  38\n",
            "Accuracy on test data:  73.53956452469464\n",
            "step: 8600 - loss:  0.05460972\n",
            "step: 8620 - loss:  0.08643625\n",
            "step: 8640 - loss:  0.07104922\n",
            "step: 8660 - loss:  0.021437587\n",
            "step: 8680 - loss:  0.026004696\n",
            "step: 8700 - loss:  0.012734601\n",
            "step: 8720 - loss:  0.01725948\n",
            "step: 8740 - loss:  0.028259011\n",
            "step: 8760 - loss:  0.01337286\n",
            "step: 8780 - loss:  0.034127902\n",
            "step: 8800 - loss:  0.04432811\n",
            "Epoch:  39\n",
            "Accuracy on test data:  73.92458842272968\n",
            "step: 8820 - loss:  0.02734113\n",
            "step: 8840 - loss:  0.030779181\n",
            "step: 8860 - loss:  0.03792914\n",
            "step: 8880 - loss:  0.017175138\n",
            "step: 8900 - loss:  0.022332137\n",
            "step: 8920 - loss:  0.018141344\n",
            "step: 8940 - loss:  0.010188067\n",
            "step: 8960 - loss:  0.025713224\n",
            "step: 8980 - loss:  0.012383283\n",
            "step: 9000 - loss:  0.039601978\n",
            "step: 9020 - loss:  0.017293734\n",
            "step: 9040 - loss:  0.06112424\n",
            "Epoch:  40\n",
            "Accuracy on test data:  74.17684545937334\n",
            "step: 9060 - loss:  0.035465125\n",
            "step: 9080 - loss:  0.056527168\n",
            "step: 9100 - loss:  0.016785298\n",
            "step: 9120 - loss:  0.027266921\n",
            "step: 9140 - loss:  0.009923679\n",
            "step: 9160 - loss:  0.004506083\n",
            "step: 9180 - loss:  0.012037638\n",
            "step: 9200 - loss:  0.016460316\n",
            "step: 9220 - loss:  0.033770964\n",
            "step: 9240 - loss:  0.0067093656\n",
            "step: 9260 - loss:  0.028114071\n",
            "Epoch:  41\n",
            "Accuracy on test data:  74.32288900690388\n",
            "step: 9280 - loss:  0.015913222\n",
            "step: 9300 - loss:  0.10442867\n",
            "step: 9320 - loss:  0.021960039\n",
            "step: 9340 - loss:  0.019452026\n",
            "step: 9360 - loss:  0.008069281\n",
            "step: 9380 - loss:  0.010134802\n",
            "step: 9400 - loss:  0.007269652\n",
            "step: 9420 - loss:  0.0074840332\n",
            "step: 9440 - loss:  0.006383175\n",
            "step: 9460 - loss:  0.020154173\n",
            "step: 9480 - loss:  0.026965536\n",
            "Epoch:  42\n",
            "Accuracy on test data:  74.41582580987786\n",
            "step: 9500 - loss:  0.016932681\n",
            "step: 9520 - loss:  0.030542731\n",
            "step: 9540 - loss:  0.015157724\n",
            "step: 9560 - loss:  0.023570852\n",
            "step: 9580 - loss:  0.008679027\n",
            "step: 9600 - loss:  0.0023789972\n",
            "step: 9620 - loss:  0.007705097\n",
            "step: 9640 - loss:  0.027404562\n",
            "step: 9660 - loss:  0.013673153\n",
            "step: 9680 - loss:  0.018613188\n",
            "step: 9700 - loss:  0.004160213\n",
            "Epoch:  43\n",
            "Accuracy on test data:  74.61497610196496\n",
            "step: 9720 - loss:  0.008528508\n",
            "step: 9740 - loss:  0.05622126\n",
            "step: 9760 - loss:  0.017135017\n",
            "step: 9780 - loss:  0.011664956\n",
            "step: 9800 - loss:  0.005705415\n",
            "step: 9820 - loss:  0.008895333\n",
            "step: 9840 - loss:  0.003173641\n",
            "step: 9860 - loss:  0.028007584\n",
            "step: 9880 - loss:  0.015600958\n",
            "step: 9900 - loss:  0.021563308\n",
            "step: 9920 - loss:  0.00832716\n",
            "step: 9940 - loss:  0.017148526\n",
            "Epoch:  44\n",
            "Accuracy on test data:  74.66808284652151\n",
            "step: 9960 - loss:  0.013035539\n",
            "step: 9980 - loss:  0.017161932\n",
            "step: 10000 - loss:  0.020065302\n",
            "step: 10020 - loss:  0.02008223\n",
            "step: 10040 - loss:  0.006845118\n",
            "step: 10060 - loss:  0.0063260174\n",
            "step: 10080 - loss:  0.005563885\n",
            "step: 10100 - loss:  0.0049152067\n",
            "step: 10120 - loss:  0.007338279\n",
            "step: 10140 - loss:  0.014744132\n",
            "step: 10160 - loss:  0.013953634\n",
            "Epoch:  45\n",
            "Accuracy on test data:  74.74774296335634\n",
            "step: 10180 - loss:  0.019316083\n",
            "step: 10200 - loss:  0.016293857\n",
            "step: 10220 - loss:  0.012384627\n",
            "step: 10240 - loss:  0.005690587\n",
            "step: 10260 - loss:  0.00934657\n",
            "step: 10280 - loss:  0.0047364323\n",
            "step: 10300 - loss:  0.0035944893\n",
            "step: 10320 - loss:  0.0147464685\n",
            "step: 10340 - loss:  0.004325503\n",
            "step: 10360 - loss:  0.0048165424\n",
            "step: 10380 - loss:  0.0114497375\n",
            "Epoch:  46\n",
            "Accuracy on test data:  74.97344662772171\n",
            "step: 10400 - loss:  0.00980621\n",
            "step: 10420 - loss:  0.0121561205\n",
            "step: 10440 - loss:  0.03888839\n",
            "step: 10460 - loss:  0.005185909\n",
            "step: 10480 - loss:  0.006961605\n",
            "step: 10500 - loss:  0.0032413169\n",
            "step: 10520 - loss:  0.0021097434\n",
            "step: 10540 - loss:  0.0050077774\n",
            "step: 10560 - loss:  0.004804034\n",
            "step: 10580 - loss:  0.012261074\n",
            "step: 10600 - loss:  0.008628841\n",
            "step: 10620 - loss:  0.011601117\n",
            "Epoch:  47\n",
            "Accuracy on test data:  74.92033988316516\n",
            "step: 10640 - loss:  0.026082942\n",
            "step: 10660 - loss:  0.009572494\n",
            "step: 10680 - loss:  0.0068867654\n",
            "step: 10700 - loss:  0.012021368\n",
            "step: 10720 - loss:  0.002964192\n",
            "step: 10740 - loss:  0.004208509\n",
            "step: 10760 - loss:  0.0026768234\n",
            "step: 10780 - loss:  0.011986434\n",
            "step: 10800 - loss:  0.005711107\n",
            "step: 10820 - loss:  0.010742417\n",
            "step: 10840 - loss:  0.004422212\n",
            "Epoch:  48\n",
            "Accuracy on test data:  75.02655337227829\n",
            "step: 10860 - loss:  0.008622746\n",
            "step: 10880 - loss:  0.01132192\n",
            "step: 10900 - loss:  0.016261388\n",
            "step: 10920 - loss:  0.0045619025\n",
            "step: 10940 - loss:  0.005333338\n",
            "step: 10960 - loss:  0.002680419\n",
            "step: 10980 - loss:  0.0038678104\n",
            "step: 11000 - loss:  0.004198049\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBl9eqcuHEz5"
      },
      "source": [],
      "execution_count": 8,
      "outputs": []
    }
  ]
}