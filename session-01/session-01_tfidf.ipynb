{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd0ef7f9a8012d9131766e31894c279374cc63c73121ed4db3b9e67a294a4bf0e74",
   "display_name": "Python 3.8.3 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'..\\datasets\\20news-bydate'\n",
    "\n",
    "def gather_20newgroup_datas():\n",
    "    dirs = [path + '\\\\' + dir_name + '\\\\' for dir_name in os.listdir(path) if not os.path.isfile(path + dir_name)]\n",
    "    train_dir, test_dir = (dirs[0], dirs[1]) if 'train' in dirs[0] else (dirs[1], dirs[0])\n",
    "    newsgroups_list = [newgroup for newgroup in os.listdir(train_dir)]\n",
    "    newsgroups_list.sort()\n",
    "\n",
    "    return train_dir, test_dir, newsgroups_list\n",
    "\n",
    "train_dir, test_dir, newsgroups_list = gather_20newgroup_datas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + '\\\\stop_words.txt', 'r') as f:\n",
    "    stop_words = f.read().split('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def collect_data_from(parent_dir, newsgroups_list):\n",
    "    data = []\n",
    "    for group_id, newsgroup in enumerate(newsgroups_list):\n",
    "        label = group_id\n",
    "        dir_path = parent_dir + newsgroup + '\\\\'\n",
    "        files = [(filename, dir_path + filename) for filename in os.listdir(dir_path) if os.path.isfile(dir_path + filename)]\n",
    "        files.sort()\n",
    "\n",
    "        for filename, filepath in files:\n",
    "            with open(filepath, 'r') as f:\n",
    "                text = f.read().lower()\n",
    "                words = [stemmer.stem(word) for word in re.split('\\W+', text) if word not in stop_words]\n",
    "\n",
    "                content = ' '.join(words)\n",
    "                assert len(content.splitlines()) == 1\n",
    "                data.append(str(label) + '<fff>' + filename + '<fff>' + content)\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "train_data = collect_data_from(train_dir, newsgroups_list)\n",
    "test_data = collect_data_from(test_dir, newsgroups_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['0<fff>49960<fff>mathew mathew manti uk subject alt atheism faq atheist resourc summari book address music atheism keyword faq atheism book music fiction address contact expir thu 29 apr 1993 11 57 19 gmt distribut organ manti consult cambridg uk supersed 19930301143317 manti uk line 290 archiv atheism resourc alt atheism archiv resourc modifi 11 decemb 1992 version 1 0 atheist resourc address atheist organ usa freedom religion foundat darwin fish bumper sticker assort atheist paraphernalia freedom religion foundat write ffrf box 750 madison wi 53701 telephon 608 256 8900 evolut design evolut design sell darwin fish fish symbol christian stick car feet word darwin written insid delux mould 3d plastic fish 4 95 postpaid write evolut design 7119 laurel canyon 4 north hollywood 91605 peopl san francisco bay area darwin fish lynn gold mail figmo netcom net peopl lynn directli price 4 95 fish american atheist press aap publish atheist book critiqu bibl list biblic contradict book bibl handbook ball foot american atheist press 372 isbn 0 910309 26 4 2nd edit 1986 bibl contradict absurd atroc immor ball foot bibl contradict aap base king jame version bibl write american atheist press box 140195 austin tx 78714 0195 7215 cameron road austin tx 78752 2973 telephon 512 458 1244 fax 512 467 9525 prometheu book sell book includ haught holi horror write 700 east amherst street buffalo york 14215 telephon 716 837 2475 altern address newer older prometheu book 59 glenn drive buffalo ny 14228 2197 african american human organ promot black secular human uncov histori black freethought publish quarterli newslett aah examin write norm allen jr african american human box 664 buffalo ny 14226 unit kingdom rationalist press associ nation secular societi 88 islington high street 702 holloway road london n1 8ew london n19 3nl 071 226 7251 071 272 1266 british humanist associ south place ethic societi 14 lamb conduit passag conway hall london wc1r 4rh red lion squar 071 430 0908 london wc1r 4rl fax 071 430 1271 071 831 7723 nation secular societi publish freethink monthli magazin found 1881 germani ibka international bund der konfessionslosen und atheisten postfach 880 1000 berlin 41 germani ibka publish journal miz materialien und informationen zur zeit politisch journal der konfessionslosesn und atheisten hrsg ibka miz vertrieb postfach 880 1000 berlin 41 germani atheist book write ibdk international ucherdienst der konfessionslosen postfach 3005 3000 hannov 1 germani telephon 0511 211216 book fiction thoma disch santa clau compromis short stori ultim proof santa exist charact event fictiti similar live dead god uh well walter miller jr canticl leibowitz gem post atom doomsday novel monk spent live copi blueprint saint leibowitz fill sheet paper ink leav white line letter edgar pangborn davi post atom doomsday novel set cleric state church exampl forbid produc describ substanc atom philip dick philip dick dick wrote philosoph thought provok short stori novel stori bizarr time approach wrote sf wrote peopl truth religion technolog believ met sort god remain sceptic novel relev galact pot healer fallibl alien deiti summon group earth craftsmen women remot planet rais giant cathedr beneath ocean deiti demand faith earther pot healer joe fernwright unabl compli polish iron amus novel maze death noteworthi descript technolog base religion vali schizophren hero search hidden mysteri gnostic christian realiti fire brain pink laser beam unknown divin origin accompani dogmat dismiss atheist friend assort odd charact divin invas god invad earth make young woman pregnant return star system termin ill assist dead man brain wire 24 hour easi listen music margaret atwood handmaid tale stori base premis congress mysteri assassin fundamentalist charg nation set book diari woman life live christian theocraci women properti revok bank account close sin luxuri outlaw radio read bibl crime punish retroact doctor perform legal abort hunt hang atwood write style difficult tale grow chill author bibl dull rambl work critic worth read ll fuss exist version sure true version book fiction peter de rosa vicar christ bantam press 1988 de rosa christian cathol enlight histori papal immor adulteri fallaci german translat gott erst diener die dunkl seit de papsttum droemer knaur 1989 michael martin atheism philosoph justif templ univers press philadelphia usa detail scholarli justif atheism outstand appendix defin terminolog usag tendenti area argu neg atheism belief exist god posit atheism belief exist god includ great refut challeng argument god attent paid refut contempori theist platinga swinburn 541 isbn 0 87722 642 3 hardcov paperback case christian templ univers press comprehens critiqu christian consid best contemporari defenc christian ultim demonstr unsupport incoher 273 isbn 0 87722 767 5 jame turner god creed john hopkin univers press baltimor md usa subtitl origin unbelief america examin unbelief agnost atheist mainstream altern view focuss period 1770 1900 consid franc britain emphasi american england develop religi histori secular atheism god creed intellectu histori fate singl idea belief god exist 316 isbn hardcov 0 8018 2494 paper 0 8018 3407 4 georg seld editor great thought ballantin book york usa dictionari quotat kind concentr statement write explicitli implicitli person philosophi view includ obscur suppress opinion peopl popular observ trace peopl express twist idea centuri number quotat deriv cardiff great men religion noy view religion 490 isbn paper 0 345 29887 richard swinburn exist god revis edit clarendon paperback oxford book second volum trilog began coher theism 1977 conclud faith reason 1981 work swinburn attempt construct seri induct argument exist god argument tendenti reli imput late 20th centuri western christian valu aesthet god supposedli simpl conceiv decis reject macki miracl theism revis edit exist god swinburn includ appendix incoher attempt rebut macki macki miracl theism oxford posthum volum comprehens review princip argument exist god rang classic philosoph posit descart anselm berkeley hume al moral argument newman kant sidgwick restat classic these plantinga swinburn address posit push concept god realm ration kierkegaard kung philip well replac god leli axiarch book delight read formalist better written martin work refreshingli direct compar hand wave swinburn jame haught holi horror illustr histori religi murder mad prometheu book religi persecut ancient time day christian librari congress catalog card number 89 64079 1990 norm allen jr african american human antholog list african american human gordon stein antholog atheism ration prometheu book antholog cover wide rang subject includ devil evil moral histori freethought comprehens bibliographi edmund cohen mind bibl believ prometheu book studi peopl christian fundamentalist net resourc small mail base archiv server manti uk carri archiv alt atheism moder articl assort file send mail archiv server manti uk help send atheism will mail repli mathew Ã¿ ']\n"
     ]
    }
   ],
   "source": [
    "print(train_data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "11314\n7532\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = train_data + test_data\n",
    "with open(path + '\\\\20news-train-processed.txt', 'w') as f:\n",
    "    f.write('\\n'.join(train_data))\n",
    "with open(path + '\\\\20news-test-processed.txt', 'w') as f:\n",
    "    f.write('\\n'.join(test_data))\n",
    "with open(path + '\\\\20news-full-processed.txt', 'w') as f:\n",
    "    f.write('\\n'.join(full_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vocabulary(data_path):\n",
    "    def compute_idf(df, corpus_size):\n",
    "        assert df > 0\n",
    "        return np.log10(corpus_size * 1. / df)\n",
    "\n",
    "    with open(data_path) as f:\n",
    "        lines = f.read().splitlines()\n",
    "    doc_count = defaultdict(int)\n",
    "    corpus_size = len(lines)\n",
    "\n",
    "    for line in lines:\n",
    "        features = line.split('<fff>')\n",
    "        text = features[-1]\n",
    "        words = list(set(text.split()))\n",
    "        for word in words:\n",
    "            doc_count[word] += 1\n",
    "\n",
    "    words_idfs = [(word, compute_idf(doc_freq, corpus_size)) for word, doc_freq in zip(doc_count.keys(), doc_count.values()) if doc_freq > 10 and not word.isdigit()]\n",
    "    words_idfs.sort(key = lambda word_idf: -word_idf[-1])\n",
    "    print('Vocabulary size: ', len(words_idfs))\n",
    "    \n",
    "    #create a dictionary\n",
    "    with open(path + '\\\\words_idfs.txt', 'w') as f:\n",
    "        f.write('\\n'.join([word + '<fff>' + str(idf) for word, idf in words_idfs]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocabulary size:  13973\n"
     ]
    }
   ],
   "source": [
    "generate_vocabulary(data_path = path + '\\\\20news-full-processed.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_idf(data_path, dataset):\n",
    "    with open(path + '\\\\words_idfs.txt') as f:\n",
    "        words_idfs = [(line.split('<fff>')[0], float(line.split('<fff>')[1])) for line in f.read().splitlines()]\n",
    "        words_ids = dict([(word, index) for index, (word, _) in enumerate(words_idfs)])\n",
    "        idfs = dict(words_idfs)\n",
    "    \n",
    "    with open(data_path) as f:\n",
    "        documents = [(int(line.split('<fff>')[0]), int(line.split('<fff>')[1]), line.split('<fff>')[2]) for line in f.read().splitlines()]\n",
    "    \n",
    "    data_tf_idf = []\n",
    "    for document in documents:\n",
    "        label, doc_id, text = document\n",
    "        words = [word for word in text.split() if word in idfs]\n",
    "        words_set = list(set(words))\n",
    "        max_term_freq = max([words.count(word) for word in words_set])\n",
    "        words_tfidfs = []\n",
    "        sum_squares = 0.0\n",
    "        for word in words_set:\n",
    "            term_freq = words.count(word)\n",
    "            tf_idf_value = term_freq * 1. / max_term_freq * idfs[word]\n",
    "            words_tfidfs.append((words_ids[word], tf_idf_value))\n",
    "            sum_squares += tf_idf_value ** 2\n",
    "        \n",
    "        words_tfidfs_normalized = [str(index) + ':' + str(tf_idf_value / np.sqrt(sum_squares)) for index, tf_idf_value in words_tfidfs]\n",
    "\n",
    "        sparse_sep = ' '.join(words_tfidfs_normalized)\n",
    "        data_tf_idf.append((label, doc_id, sparse_sep))\n",
    "\n",
    "    with open(path + '\\\\20news_' + dataset + '_tfidf.txt', 'w') as f:\n",
    "            f.write('\\n'.join([str(label) + '<fff>' + str(doc_id) + '<fff>' + sparse_sep for (label, doc_id, sparse_sep) in data_tf_idf]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tf_idf(data_path = path + '\\\\20news-full-processed.txt', dataset = 'full')\n",
    "get_tf_idf(data_path = path + '\\\\20news-train-processed.txt', dataset = 'train')\n",
    "get_tf_idf(data_path = path + '\\\\20news-test-processed.txt', dataset = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}