{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '..\\datasets\\death_rates_data.txt'\n",
    "\n",
    "def get_data(path = path):\n",
    "    if os.path.exists(path):\n",
    "        X = []\n",
    "        y = []\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                sample = [float(elem) for elem in line.split()]\n",
    "                X.append(sample[:-1])\n",
    "                y.append(sample[-1])\n",
    "        return X, y\n",
    "    else:\n",
    "        print('this path doesn''t exist!')\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_add_ones(X):\n",
    "    X = np.array(X)\n",
    "    X_max = np.max(X, axis = 0)\n",
    "    X_min = np.min(X, axis = 0)\n",
    "\n",
    "    normalized_X = (X - X_min) / (X_max - X_min)\n",
    "    ones = np.ones((X.shape[0], 1))\n",
    "    return np.hstack((ones, normalized_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ridge_regression:\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def fit(self, X_train, y_train, LAMBDA):\n",
    "        assert len(X_train.shape) == 2 and X_train.shape[0] == y_train.shape[0]\n",
    "        return (np.linalg.inv(X_train.transpose().dot(X_train)) + LAMBDA * np.identity(X_train.shape[1])).dot(X_train.transpose()).dot(y_train)\n",
    "    \n",
    "    def fit_grad_descent(self, X_train, y_train, LAMBDA, learning_rate, max_num_epochs = 100, batch_size = 128):\n",
    "        pass\n",
    "\n",
    "    def predict(self, W, x_new):\n",
    "        x_new = np.array(x_new)\n",
    "        y_new = x_new.dot(W)\n",
    "        return y_new\n",
    "        \n",
    "    def compute_RSS(self, y_new, y_predicted):\n",
    "        loss =  (1. / y_new.shape[0]) * np.sum ((y_new - y_predicted) ** 2)\n",
    "        return loss\n",
    "\n",
    "    def get_the_best_LAMBDA(self, X_train, y_train):\n",
    "        def cross_validation(num_folds, LAMBDA):\n",
    "            row_ids = np.array(range(X_train.shape[0]))\n",
    "\n",
    "            valid_ids = np.split(row_ids[:len(row_ids) - len(row_ids) % num_folds], num_folds)\n",
    "            valid_ids[-1] = np.append(valid_ids[-1], row_ids[len(row_ids) - len(row_ids) % num_folds:])\n",
    "\n",
    "            train_ids = [[k for k in row_ids if k not in valid_ids[i]] for i in range(num_folds)]\n",
    "\n",
    "            aver_RSS = 0\n",
    "            for i in range(num_folds):\n",
    "                valid_part = {'X': np.array([X_train[j] for j in valid_ids[i]]), 'y': np.array([y_train[j] for j in valid_ids[i]])}\n",
    "                train_part = {'X': np.array([X_train[j] for j in train_ids[i]]), 'y': np.array([y_train[j] for j in train_ids[i]])}\n",
    "                W = self.fit(train_part['X'], train_part['y'], LAMBDA)\n",
    "                y_predicted = self.predict(W, valid_part['X'])\n",
    "                aver_RSS += self.compute_RSS(valid_part['y'], y_predicted)\n",
    "\n",
    "            return aver_RSS / num_folds\n",
    "\n",
    "        def range_scan(best_LAMBDA, minimum_RSS, LAMBDA_values):\n",
    "            for current_LAMBDA in LAMBDA_values:\n",
    "                aver_RSS = cross_validation(5, current_LAMBDA)\n",
    "                if aver_RSS < minimum_RSS:\n",
    "                    best_LAMBDA = current_LAMBDA\n",
    "                    minimum_RSS = aver_RSS\n",
    "            return best_LAMBDA, minimum_RSS\n",
    "\n",
    "        best_LAMBDA, minimum_RSS = range_scan(0, minimum_RSS = 10000 ** 2, LAMBDA_values = range(50))\n",
    "\n",
    "        LAMBDA_values = [k * 1. / 1000 for k in range(max(0, (best_LAMBDA - 1) * 1000), best_LAMBDA * 1000 + 1, 1)]\n",
    "        \n",
    "        best_LAMBDA, minimum_RSS = range_scan(best_LAMBDA, minimum_RSS = minimum_RSS, LAMBDA_values = LAMBDA_values)\n",
    "        \n",
    "        return best_LAMBDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "best lambda:  0\nloss:  2191.979180191641\n"
     ]
    }
   ],
   "source": [
    "X, y = get_data()\n",
    "X = normalize_and_add_ones(X)\n",
    "y = np.array(y)\n",
    "X_train, y_train = X[:50], y[:50]\n",
    "X_test, y_test = X[50:], y[50:]\n",
    "\n",
    "ridge_regressor = Ridge_regression()\n",
    "\n",
    "best_LAMBDA = ridge_regressor.get_the_best_LAMBDA(X_train, y_train)\n",
    "print('best lambda: ', best_LAMBDA)\n",
    "\n",
    "W_learned = ridge_regressor.fit(X_train, y_train, best_LAMBDA)\n",
    "y_predicted = ridge_regressor.predict(W_learned, X_test)\n",
    "\n",
    "rss = ridge_regressor.compute_RSS(y_test, y_predicted)\n",
    "print('loss: ', rss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}